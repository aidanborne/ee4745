{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrbKnHDZ__lZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load image via pytorch"
      ],
      "metadata": {
        "id": "L6ve6Qd3Lfe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "br51ZxVkLOrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(root='./data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=64,\n",
        "                                           shuffle=True)"
      ],
      "metadata": {
        "id": "Vsom0-pxLmRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Remove all in valid files that is not end with jpg\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=\"data/sample-folder/train\", transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "5Epq4zuhNX3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "447fe5cc"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "mnist_df = pd.read_csv('/content/sample_data/mnist_train_small.csv')\n",
        "\n",
        "# Display the first 5 rows\n",
        "display(mnist_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38acdb59"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.labels = dataframe.iloc[:, 0].values\n",
        "        self.images = dataframe.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.from_numpy(self.images[idx]).float().unsqueeze(0) / 255.0\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return image, label\n",
        "\n",
        "# Create the dataset\n",
        "mnist_dataset = MNISTDataset(mnist_df)\n",
        "\n",
        "# Create the DataLoader\n",
        "mnist_dataloader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "print(\"PyTorch Dataset and DataLoader created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d5c52b1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "# Function to show a batch of images\n",
        "def show_images(dataloader):\n",
        "    dataiter = iter(dataloader)\n",
        "    images, labels = next(dataiter)\n",
        "    img_grid = torchvision.utils.make_grid(images)\n",
        "    img_grid = img_grid.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img_grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a batch of training data\n",
        "show_images(mnist_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 0) Environment & Imports\\\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "torch.manual_seed(42)\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "OUT = Path(\"./outputs\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torchvision:\", torchvision.__version__)\n"
      ],
      "metadata": {
        "id": "TiDOyTl_RkyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 1) Define LeNet-5 (for MNIST, 32x32 input)\n",
        "#    Classic: conv(6,5x5)->pool->conv(16,5x5)->pool->FC(120)->FC(84)->FC(10)\n",
        "# =========================================================\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)   # 1x32x32 -> 6x28x28\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                 # 6x28x28 -> 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)  # 6x14x14 -> 16x10x10\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                 # 16x10x10 -> 16x5x5\n",
        "\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)  # 400 -> 120\n",
        "        self.fc2 = nn.Linear(120, 84)      # 120 -> 84\n",
        "        self.fc3 = nn.Linear(84, num_classes)  # 84 -> 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  # conv1 out\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))  # conv2 out\n",
        "        x = self.pool2(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "model = LeNet5().eval()\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "W0HQUj6S6iyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# DataLoader\n",
        "train_set = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "    acc = correct / len(train_set)\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_set):.4f}, acc={acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "fkYxZDwaR1Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Data: MNIST (resize to 32x32)\n",
        "# =========================================================\n",
        "test_set = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
        "\n",
        "sample_img, sample_label = next(iter(test_loader))\n",
        "print(\"Sample shape:\", sample_img.shape, \"Label:\", sample_label.item())\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.title(f\"MNIST sample (label={sample_label.item()})\")\n",
        "plt.axis('off')\n",
        "plt.imshow(sample_img[0,0].numpy(), cmap='gray')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7iUd_7at6olQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3) Feature Map Visualization via Hooks (micro-level)\n",
        "#    - Capture outputs of conv1 and conv2\n",
        "# =========================================================\n",
        "model.to('cpu')\n",
        "feature_maps = {}\n",
        "\n",
        "def get_hook(name):\n",
        "    def hook(module, input, output):\n",
        "        # output: [B, C, H, W]\n",
        "        feature_maps[name] = output.detach().cpu()\n",
        "    return hook\n",
        "\n",
        "h1 = model.conv1.register_forward_hook(get_hook(\"conv1\"))\n",
        "h2 = model.conv2.register_forward_hook(get_hook(\"conv2\"))\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = model(sample_img)\n",
        "\n",
        "h1.remove()\n",
        "h2.remove()\n",
        "\n",
        "def visualize_feature_maps(fm, n_cols=8, title=\"Feature Maps\"):\n",
        "    \"\"\"\n",
        "    fm: Tensor [B, C, H, W], we visualize the first sample\n",
        "    \"\"\"\n",
        "    fm = fm[0]  # [C, H, W]\n",
        "    C = fm.shape[0]\n",
        "    n_rows = int(np.ceil(C / n_cols))\n",
        "    fm_min = fm.min(dim=1, keepdim=True)[0].min(dim=2, keepdim=True)[0]\n",
        "    fm_max = fm.max(dim=1, keepdim=True)[0].max(dim=2, keepdim=True)[0]\n",
        "    fm_norm = (fm - fm_min) / (fm_max - fm_min + 1e-6)\n",
        "\n",
        "    grid = make_grid(fm_norm.unsqueeze(1), nrow=n_cols, padding=1, normalize=False)\n",
        "    plt.figure(figsize=(1.6*n_cols, 1.6*n_rows))\n",
        "    plt.suptitle(title)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid.permute(1,2,0).numpy().squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "visualize_feature_maps(feature_maps[\"conv1\"], n_cols=6, title=\"conv1 feature maps (6 channels)\")\n",
        "visualize_feature_maps(feature_maps[\"conv2\"], n_cols=8, title=\"conv2 feature maps (16 channels)\")\n"
      ],
      "metadata": {
        "id": "koY4W99D6vuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model in pt\n",
        "dummy_input = torch.randn(1, 1, 32, 32)\n",
        "# trace model\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "# save model\n",
        "traced_model.save('lenet5.pt')\n"
      ],
      "metadata": {
        "id": "oPmpEuoE6yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# =========================================================\n",
        "# 1) Load pretrained AlexNet\n",
        "# =========================================================\n",
        "\n",
        "model = alexnet(pretrained=True).to(DEVICE).eval()\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
        "])\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "fmnmIEom74ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Load an image (upload or use sample)\n",
        "# =========================================================\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "if len(uploaded) > 0:\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    img = Image.open(fname).convert('RGB')\n",
        "else:\n",
        "    w, h = 640, 400\n",
        "    arr = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for i in range(h):\n",
        "        arr[i,:,0] = np.clip(255 * (i/h), 0, 255)\n",
        "        arr[i,:,1] = np.clip(255 * (1 - i/h), 0, 255)\n",
        "        arr[i,:,2] = 128\n",
        "    img = Image.fromarray(arr, 'RGB')\n",
        "\n",
        "plt.figure(figsize=(4,4)); plt.imshow(img); plt.axis('off'); plt.title(\"Input image\")\n",
        "plt.show()\n",
        "\n",
        "inp = preprocess(img).unsqueeze(0).to(DEVICE)  # [1,3,224,224]\n"
      ],
      "metadata": {
        "id": "K_mg4fz8BOij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3) Choose layers to hook\n",
        "#    AlexNet.features\n",
        "#    0:Conv1, 1:ReLU, 2:MaxPool,\n",
        "#    3:Conv2, 4:ReLU, 5:MaxPool,\n",
        "#    6:Conv3, 7:ReLU,\n",
        "#    8:Conv4, 9:ReLU,\n",
        "#    10:Conv5, 11:ReLU, 12:MaxPool\n",
        "# =========================================================\n",
        "layer_indices = [0,3,6,8,10]\n",
        "layer_names = {\n",
        "    0: \"conv1\", 1: \"relu1\", 2: \"pool1\",\n",
        "    3: \"conv2\", 4: \"relu2\", 5: \"pool2\",\n",
        "    6: \"conv3\", 7: \"relu3\",\n",
        "    8: \"conv4\", 9: \"relu4\",\n",
        "    10:\"conv5\", 11:\"relu5\", 12:\"pool5\"\n",
        "}\n",
        "\n",
        "feature_maps = {}\n",
        "\n",
        "def make_hook(name):\n",
        "    def hook(m, x, y):\n",
        "        # y: [B,C,H,W]\n",
        "        feature_maps[name] = y.detach().float().cpu()\n",
        "    return hook\n",
        "\n",
        "handles = []\n",
        "for idx in layer_indices:\n",
        "    m = model.features[idx]\n",
        "    handles.append(m.register_forward_hook(make_hook(layer_names[idx])))\n",
        "\n",
        "# Forward once\n",
        "with torch.no_grad():\n",
        "    _ = model(inp)\n",
        "\n",
        "# clear hook\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "list(feature_maps.keys())\n"
      ],
      "metadata": {
        "id": "FpsAhTMvBfcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4) Utilities: visualize & save feature map grids\n",
        "# =========================================================\n",
        "from torchvision.utils import make_grid\n",
        "OUTDIR = Path('./alexnet_fmaps'); OUTDIR.mkdir(exist_ok=True, parents=True)\n",
        "def minmax_norm(t, eps=1e-6):\n",
        "    # t: [H,W]\n",
        "    t_min = t.amin(dim=(-2,-1), keepdim=True)\n",
        "    t_max = t.amax(dim=(-2,-1), keepdim=True)\n",
        "    return (t - t_min) / (t_max - t_min + eps)\n",
        "\n",
        "def save_fmap_grid(t, title, ncols=8, limit=None):\n",
        "    \"\"\"\n",
        "    t: Tensor [B, C, H, W]\n",
        "    \"\"\"\n",
        "    t = t[0]  # [C,H,W]\n",
        "    C, H, W = t.shape\n",
        "    if limit is not None:\n",
        "        C = min(C, limit)\n",
        "        t = t[:C]\n",
        "\n",
        "    normed = []\n",
        "    for c in range(C):\n",
        "        x = t[c]\n",
        "        x = (x - x.min()) / (x.max() - x.min() + 1e-6)\n",
        "        normed.append(x.unsqueeze(0))  # [1,H,W]\n",
        "    grid = make_grid(torch.stack(normed, dim=0), nrow=ncols, padding=2)  # [3?,H',W'] 但我们给的是单通道，所以会是 [C',H',W']\n",
        "\n",
        "    plt.figure(figsize=(1.6*ncols, 1.6*math.ceil(C/ncols)))\n",
        "    plt.suptitle(title)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # save\n",
        "    outpath = OUTDIR / f\"{title.replace(' ','_')}.png\"\n",
        "    plt.imsave(outpath.as_posix(), grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "for k in feature_maps:\n",
        "    ch = feature_maps[k].shape[1]\n",
        "    limit = 32 if ch > 32 else None\n",
        "    save_fmap_grid(feature_maps[k], title=f\"{k} ({ch}ch)\", ncols=8, limit=limit)\n"
      ],
      "metadata": {
        "id": "sF2zSBkgBmXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 5) Make a prediction and show the class name\n",
        "# =========================================================\n",
        "# Load ImageNet class labels\n",
        "import json\n",
        "from google.colab import data_table\n",
        "import requests\n",
        "\n",
        "# Updated URL for ImageNet class labels\n",
        "class_labels_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "response = requests.get(class_labels_url)\n",
        "class_labels = [line.strip() for line in response.text.split('\\n') if line.strip()]\n",
        "\n",
        "# Make prediction\n",
        "model.to(DEVICE) # Move model back to the device if it was moved to cpu\n",
        "with torch.no_grad():\n",
        "    outputs = model(inp)\n",
        "    _, predicted = outputs.max(1)\n",
        "    predicted_class_index = predicted.item()\n",
        "\n",
        "# Get the predicted class name\n",
        "predicted_class_name = class_labels[predicted_class_index]\n",
        "\n",
        "print(f\"Predicted class index: {predicted_class_index}\")\n",
        "print(f\"Predicted class name: {predicted_class_name}\")"
      ],
      "metadata": {
        "id": "Dnh9XxsjjfPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 5) Optional: also visualize first-layer kernels directly\n",
        "# =========================================================\n",
        "w = model.features[0].weight.detach().cpu()  # [64,3,11,11]\n",
        "\n",
        "kernels = w.mean(1, keepdim=True)  # [64,1,11,11]\n",
        "kernels = (kernels - kernels.min()) / (kernels.max() - kernels.min() + 1e-6)\n",
        "grid = make_grid(kernels, nrow=8, padding=1)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.title(\"conv1 kernels (channel-mean)\")\n",
        "plt.axis('off')\n",
        "plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "plt.imsave((OUTDIR/\"conv1_kernels.png\").as_posix(), grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "print(\"Saved:\", OUTDIR/\"conv1_kernels.png\")\n"
      ],
      "metadata": {
        "id": "epJ8sgRTBswT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model pt\n",
        "model.to('cpu')\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "# trace model\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "# save model\n",
        "traced_model.save('alexnet.pt')\n"
      ],
      "metadata": {
        "id": "BfNRYEGWB8jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import inception_v3\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# =========================================================\n",
        "# 1) Load pretrained Inception\n",
        "# =========================================================\n",
        "\n",
        "model = inception_v3(pretrained=True).to(DEVICE).eval()\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
        "])\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "8Qq7nC-JrCpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Load an image (upload or use sample) - Using the same image loading code as before\n",
        "# =========================================================\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "if len(uploaded) > 0:\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    img = Image.open(fname).convert('RGB')\n",
        "else:\n",
        "    w, h = 640, 400\n",
        "    arr = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for i in range(h):\n",
        "        arr[i,:,0] = np.clip(255 * (i/h), 0, 255)\n",
        "        arr[i,:,1] = np.clip(255 * (1 - i/h), 0, 255)\n",
        "        arr[i,:,2] = 128\n",
        "    img = Image.fromarray(arr, 'RGB')\n",
        "\n",
        "plt.figure(figsize=(4,4)); plt.imshow(img); plt.axis('off'); plt.title(\"Input image\")\n",
        "plt.show()\n",
        "\n",
        "inp = preprocess(img).unsqueeze(0).to(DEVICE)  # [1,3,224,224]\n",
        "\n",
        "# =========================================================\n",
        "# 3) Choose layers to hook\n",
        "#    Inception V3 layers: Conv2d_1a_3x3, Conv2d_2a_3x3, Conv2d_2b_3x3, Mixed_5b, Mixed_5c, Mixed_5d, Mixed_6a, Mixed_6b, Mixed_6c, Mixed_6d, Mixed_6e, Mixed_7a, Mixed_7b, Mixed_7c\n",
        "# =========================================================\n",
        "layer_names = ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c']\n",
        "\n",
        "feature_maps = {}\n",
        "\n",
        "def make_hook(name):\n",
        "    def hook(m, x, y):\n",
        "        # y: [B,C,H,W]\n",
        "        feature_maps[name] = y.detach().float().cpu()\n",
        "    return hook\n",
        "\n",
        "handles = []\n",
        "for name in layer_names:\n",
        "    m = getattr(model, name)\n",
        "    handles.append(m.register_forward_hook(make_hook(name)))\n",
        "\n",
        "# Forward once\n",
        "with torch.no_grad():\n",
        "    _ = model(inp)\n",
        "\n",
        "# clear hook\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "list(feature_maps.keys())"
      ],
      "metadata": {
        "id": "GVbVQtjSrKdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qr0zCjdlrjSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6264592b"
      },
      "source": [
        "# =========================================================\n",
        "# 4) Utilities: visualize & save feature map grids\n",
        "# =========================================================\n",
        "from torchvision.utils import make_grid\n",
        "OUTDIR = Path('./inception_fmaps'); OUTDIR.mkdir(exist_ok=True, parents=True)\n",
        "def minmax_norm(t, eps=1e-6):\n",
        "    # t: [H,W]\n",
        "    t_min = t.amin(dim=(-2,-1), keepdim=True)\n",
        "    t_max = t.amax(dim=(-2,-1), keepdim=True)\n",
        "    return (t - t_min) / (t_max - t_min + eps)\n",
        "\n",
        "def save_fmap_grid(t, title, ncols=8, limit=None):\n",
        "    \"\"\"\n",
        "    t: Tensor [B, C, H, W]\n",
        "    \"\"\"\n",
        "    t = t[0]  # [C,H,W]\n",
        "    C, H, W = t.shape\n",
        "    if limit is not None:\n",
        "        C = min(C, limit)\n",
        "        t = t[:C]\n",
        "\n",
        "    normed = []\n",
        "    for c in range(C):\n",
        "        x = t[c]\n",
        "        x = (x - x.min()) / (x.max() - x.min() + 1e-6)\n",
        "        normed.append(x.unsqueeze(0))  # [1,H,W]\n",
        "    grid = make_grid(torch.stack(normed, dim=0), nrow=ncols, padding=2)  # [3?,H',W'] 但我们给的是单通道，所以会是 [C',H',W']\n",
        "\n",
        "    plt.figure(figsize=(1.6*ncols, 1.6*math.ceil(C/ncols)))\n",
        "    plt.suptitle(title)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # save\n",
        "    outpath = OUTDIR / f\"{title.replace(' ','_')}.png\"\n",
        "    plt.imsave(outpath.as_posix(), grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "# Visualize feature maps for 'Mixed_5b'\n",
        "layer_name = 'Mixed_5b'\n",
        "if layer_name in feature_maps:\n",
        "    ch = feature_maps[layer_name].shape[1]\n",
        "    limit = 32 if ch > 32 else None\n",
        "    save_fmap_grid(feature_maps[layer_name], title=f\"{layer_name} ({ch}ch)\", ncols=8, limit=limit)\n",
        "else:\n",
        "    print(f\"Feature maps for '{layer_name}' not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BJlG4hNrjIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1096b795"
      },
      "source": [
        "# =========================================================\n",
        "# 5) Make a prediction and show the class name\n",
        "# =========================================================\n",
        "# Load ImageNet class labels\n",
        "import json\n",
        "from google.colab import data_table\n",
        "import requests\n",
        "\n",
        "# Updated URL for ImageNet class labels\n",
        "class_labels_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "response = requests.get(class_labels_url)\n",
        "class_labels = [line.strip() for line in response.text.split('\\n') if line.strip()]\n",
        "\n",
        "# Make prediction\n",
        "model.to(DEVICE) # Move model back to the device if it was moved to cpu\n",
        "with torch.no_grad():\n",
        "    outputs = model(inp)\n",
        "    _, predicted = outputs.max(1)\n",
        "    predicted_class_index = predicted.item()\n",
        "\n",
        "# Get the predicted class name\n",
        "predicted_class_name = class_labels[predicted_class_index]\n",
        "\n",
        "print(f\"Predicted class index: {predicted_class_index}\")\n",
        "print(f\"Predicted class name: {predicted_class_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQ8XNLNGri3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ead8d6"
      },
      "source": [
        "# Save the model pt\n",
        "model.to('cpu')\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "# trace model\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "# save model\n",
        "traced_model.save('inception_v3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# =========================================================\n",
        "# 1) Load pretrained Resnet-18\n",
        "# =========================================================\n",
        "\n",
        "model = resnet18(pretrained=True).to(DEVICE).eval()\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
        "])\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "_L0iF4gyg5FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Load an image (upload or use sample) - Using the same image loading code as before\n",
        "# =========================================================\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "if len(uploaded) > 0:\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    img = Image.open(fname).convert('RGB')\n",
        "else:\n",
        "    w, h = 640, 400\n",
        "    arr = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for i in range(h):\n",
        "        arr[i,:,0] = np.clip(255 * (i/h), 0, 255)\n",
        "        arr[i,:,1] = np.clip(255 * (1 - i/h), 0, 255)\n",
        "        arr[i,:,2] = 128\n",
        "    img = Image.fromarray(arr, 'RGB')\n",
        "\n",
        "plt.figure(figsize=(4,4)); plt.imshow(img); plt.axis('off'); plt.title(\"Input image\")\n",
        "plt.show()\n",
        "\n",
        "inp = preprocess(img).unsqueeze(0).to(DEVICE)  # [1,3,224,224]\n",
        "\n",
        "# =========================================================\n",
        "# 3) Choose layers to hook\n",
        "#    ResNet18 layers: conv1, layer1, layer2, layer3, layer4\n",
        "# =========================================================\n",
        "layer_names = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
        "\n",
        "feature_maps = {}\n",
        "\n",
        "def make_hook(name):\n",
        "    def hook(m, x, y):\n",
        "        # y: [B,C,H,W]\n",
        "        feature_maps[name] = y.detach().float().cpu()\n",
        "    return hook\n",
        "\n",
        "handles = []\n",
        "for name in layer_names:\n",
        "    m = getattr(model, name)\n",
        "    handles.append(m.register_forward_hook(make_hook(name)))\n",
        "\n",
        "# Forward once\n",
        "with torch.no_grad():\n",
        "    _ = model(inp)\n",
        "\n",
        "# clear hook\n",
        "for h in handles:\n",
        "    h.remove()\n",
        "\n",
        "list(feature_maps.keys())"
      ],
      "metadata": {
        "id": "3hkcPGkCCAku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24efd2d3"
      },
      "source": [
        "# =========================================================\n",
        "# 4) Utilities: visualize & save feature map grids\n",
        "# =========================================================\n",
        "from torchvision.utils import make_grid\n",
        "OUTDIR = Path('./resnet_fmaps'); OUTDIR.mkdir(exist_ok=True, parents=True)\n",
        "def minmax_norm(t, eps=1e-6):\n",
        "    # t: [H,W]\n",
        "    t_min = t.amin(dim=(-2,-1), keepdim=True)\n",
        "    t_max = t.amax(dim=(-2,-1), keepdim=True)\n",
        "    return (t - t_min) / (t_max - t_min + eps)\n",
        "\n",
        "def save_fmap_grid(t, title, ncols=8, limit=None):\n",
        "    \"\"\"\n",
        "    t: Tensor [B, C, H, W]\n",
        "    \"\"\"\n",
        "    t = t[0]  # [C,H,W]\n",
        "    C, H, W = t.shape\n",
        "    if limit is not None:\n",
        "        C = min(C, limit)\n",
        "        t = t[:C]\n",
        "\n",
        "    normed = []\n",
        "    for c in range(C):\n",
        "        x = t[c]\n",
        "        x = (x - x.min()) / (x.max() - x.min() + 1e-6)\n",
        "        normed.append(x.unsqueeze(0))  # [1,H,W]\n",
        "    grid = make_grid(torch.stack(normed, dim=0), nrow=ncols, padding=2)\n",
        "    plt.figure(figsize=(1.6*ncols, 1.6*math.ceil(C/ncols)))\n",
        "    plt.suptitle(title)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # save\n",
        "    outpath = OUTDIR / f\"{title.replace(' ','_')}.png\"\n",
        "    plt.imsave(outpath.as_posix(), grid.permute(1,2,0).squeeze(), cmap='gray')\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "# Visualize feature maps for 'layer1'\n",
        "layer_name = 'conv1'\n",
        "if layer_name in feature_maps:\n",
        "    ch = feature_maps[layer_name].shape[1]\n",
        "    limit = 32 if ch > 32 else None\n",
        "    save_fmap_grid(feature_maps[layer_name], title=f\"{layer_name} ({ch}ch)\", ncols=8, limit=limit)\n",
        "else:\n",
        "    print(f\"Feature maps for '{layer_name}' not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 5) Make a prediction and show the class name\n",
        "# =========================================================\n",
        "# Load ImageNet class labels\n",
        "import json\n",
        "from google.colab import data_table\n",
        "import requests\n",
        "\n",
        "# Updated URL for ImageNet class labels\n",
        "class_labels_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "response = requests.get(class_labels_url)\n",
        "class_labels = [line.strip() for line in response.text.split('\\n') if line.strip()]\n",
        "\n",
        "# Make prediction\n",
        "model.to(DEVICE) # Move model back to the device if it was moved to cpu\n",
        "with torch.no_grad():\n",
        "    outputs = model(inp)\n",
        "    _, predicted = outputs.max(1)\n",
        "    predicted_class_index = predicted.item()\n",
        "\n",
        "# Get the predicted class name\n",
        "predicted_class_name = class_labels[predicted_class_index]\n",
        "\n",
        "print(f\"Predicted class index: {predicted_class_index}\")\n",
        "print(f\"Predicted class name: {predicted_class_name}\")"
      ],
      "metadata": {
        "id": "7X27BJdYhbIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7b313ff"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8f03edc"
      },
      "source": [
        "# Save the model pt\n",
        "model.to('cpu')\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "# trace model\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "# save model\n",
        "traced_model.save('resnet18.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}