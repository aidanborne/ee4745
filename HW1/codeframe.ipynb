{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16093f2c",
   "metadata": {},
   "source": [
    "# Machine Learning Homework 1\n",
    "**Student Name:** __Aidan Borne____  \n",
    "**LSU Email Address:** _aborn13@lsu.edu__\n",
    "## Instructions\n",
    "- This code frame is AI-generated (Claude AI Sonnet 4). If there is any problem, let me know ASAP. \n",
    "- You are NOT required to use this framework.\n",
    "- Fill in your code in the designated sections marked with `# YOUR CODE HERE`\n",
    "- Do not modify the structure of this notebook\n",
    "- Make sure all cells run without errors\n",
    "- Submit this completed notebook file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c15b9",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 1: House Price Dataset (5 pts)\n",
    "\n",
    "In this question, you will work with a real estate dataset to predict house prices using linear regression.\n",
    "\n",
    "### Dataset Description\n",
    "The HOUSES dataset contains recent real estate listings in San Luis Obispo county with the following fields:\n",
    "- **MLS**: Multiple listing service number (unique ID)\n",
    "- **Location**: City/town where the house is located\n",
    "- **Price**: Most recent listing price (in dollars)\n",
    "- **Bedrooms**: Number of bedrooms\n",
    "- **Bathrooms**: Number of bathrooms  \n",
    "- **Size**: House size in square feet\n",
    "- **Price/SQ.ft**: Price per square foot\n",
    "- **Status**: Type of sale (Short Sale, Foreclosure, Regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as SkLinearRegression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb2831",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89442bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# TODO: YOUR CODE HERE: Read the RealEstate.csv file using pandas\n",
    "data_path = './RealEstate.csv'# Your path to RealEstate.csv\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "print(f'\\nTotal number of samples: {len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa750a1b",
   "metadata": {},
   "source": [
    "### 1.2 Data Preprocessing\n",
    "\n",
    "You need to:\n",
    "1. Filter data for each status type (Short Sale, Foreclosure, Regular)\n",
    "2. Handle categorical features (Location)\n",
    "3. Normalize/standardize continuous features\n",
    "4. Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecab0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1.2.1 Filter data by status\n",
    "print(\"Available status types:\", data['Status'].unique())\n",
    "print(\"Count of each status type:\")\n",
    "print(data['Status'].value_counts())\n",
    "\n",
    "# Filter for Short Sale data (you'll need to repeat this for Foreclosure and Regular)\n",
    "short_sale_data = data[data['Status'] == 'Short Sale']\n",
    "foreclosure_data = data[data['Status'] == 'Foreclosure']\n",
    "regular_data = data[data['Status'] == 'Regular']\n",
    "\n",
    "print(f\"\\nShort Sale samples: {len(short_sale_data)}\")\n",
    "print(f\"\\nForeclosure samples: {len(foreclosure_data)}\")\n",
    "print(f\"\\nRegular samples: {len(regular_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ddaf1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1.2.2 Implement MinMaxScaler from scratch\n",
    "class MinMaxScaler:\n",
    "    \"\"\"Min-Max scaler to normalize features to range [0, 1]\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_range=(0, 1)):\n",
    "        self.min = feature_range[0]\n",
    "        self.max = feature_range[1]\n",
    "        self.data_min = None\n",
    "        self.data_max = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the scaler to the data\"\"\"\n",
    "        # TODO: YOUR CODE HERE: Calculate min and max values for each feature\n",
    "        self.data_min = data.min()\n",
    "        self.data_max = data.max()\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Transform the data using fitted parameters\"\"\"\n",
    "        # TODO: YOUR CODE HERE: Apply min-max scaling transformation\n",
    "        # Formula: (data - data_min) / (data_max - data_min) * (max - min) + min\n",
    "        scaled_data = (data - self.data_min) / (self.data_max - self.data_min)\n",
    "        scaled_data = scaled_data * (self.max - self.min) + self.min\n",
    "        return scaled_data\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db4573",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1.2.3 Handle categorical features (Location)\n",
    "# There are multiple ways to handle categorical features. Here, we'll replace each location with the average price of houses in that location.\n",
    "# This approach captures the effect of location on price while keeping the feature numerical. However, pay attention to locations in the test set that may not exist in the training set.\n",
    "# Alternatively, you could use one-hot encoding or label encoding.\n",
    "def preprocess_location_feature(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Replace location with average price for that location\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE: \n",
    "    # 1. Calculate average price for each location using training data\n",
    "    # 2. Replace location names with these average prices\n",
    "    # 3. Handle locations in test set that don't exist in training set\n",
    "    \n",
    "    location_price_map = y_train.groupby(X_train['Location']).mean().to_dict()['Price']\n",
    "\n",
    "    # I'll just use the average price for all locations\n",
    "    # if a location isn't in the training set\n",
    "    global_average = y_train.mean()['Price']\n",
    "    \n",
    "    for loc in X_test['Location'].unique():\n",
    "        if loc not in location_price_map:\n",
    "            # handle the unseen location\n",
    "            location_price_map[loc] = global_average\n",
    "    \n",
    "    # Apply the mapping\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # YOUR CODE HERE: Implement the location replacement logic\n",
    "    X_train_processed['Location'] = X_train_processed['Location'].map(location_price_map)\n",
    "    X_test_processed['Location'] = X_test_processed['Location'].map(location_price_map)\n",
    "    \n",
    "    return X_train_processed, X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306bf5b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1.2.4 Split data and preprocess features\n",
    "# TODO: YOUR CODE HERE: Split the short_sale_data into features (X) and target (y)\n",
    "X = short_sale_data.drop(columns=['Price', 'Status'])  # Features (exclude 'Price' and 'Status')\n",
    "y = short_sale_data[[\"Price\"]]  # Target ('Price')\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Process location feature\n",
    "X_train, X_test = preprocess_location_feature(X_train, X_test, y_train)\n",
    "\n",
    "# Apply min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# YOUR CODE HERE: Fit scaler on training data and transform both train and test sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Optional: Apply log transformation to target variable to reduce skewness\n",
    "# YOUR CODE HERE: Apply log1p transformation to y_train and y_test\n",
    "y_train_log = None\n",
    "y_test_log = None\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e141e",
   "metadata": {},
   "source": [
    "### 1.3 Linear Regression Implementation\n",
    "\n",
    "Implement linear regression from scratch using the closed-form solution (Normal Equation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c9d5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"Linear Regression using closed-form solution\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model using Closed-form Equation\n",
    "        \"\"\"\n",
    "        # TODO: YOUR CODE HERE: Implement the closed-form solution\n",
    "        # Steps:\n",
    "        # 1. Add bias column to X (column of ones)\n",
    "        # 2. Use Closed-form Equation: theta = (X^T * X)^(-1) * X^T * y\n",
    "        # 3. Extract bias and weights from theta\n",
    "\n",
    "        # add the bias column\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Bias'] = 1\n",
    "\n",
    "        X_np = X_copy.to_numpy()\n",
    "        X_transposed = X_np.transpose()\n",
    "\n",
    "        # closed-form solution\n",
    "        inverted = np.linalg.inv(X_transposed @ X_np)\n",
    "        theta = inverted @ X_transposed @ y.to_numpy()\n",
    "\n",
    "        # extract weights and bias (last element)\n",
    "        self.weights, self.bias = theta[:-1], theta[-1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        # TODO: YOUR CODE HERE: Implement prediction\n",
    "        # Don't forget to add bias term\n",
    "        return (X @ self.weights + self.bias).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8319e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train your custom linear regression model\n",
    "lr_model = LinearRegression()\n",
    "# TODO: YOUR CODE HERE: Fit the model and make predictions\n",
    "# Use the log-transformed target if you applied it above\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = lr_model.predict(X_test_scaled)  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71887ae8",
   "metadata": {},
   "source": [
    "### 1.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error\"\"\"\n",
    "    # YOUR CODE HERE: Implement MSE calculation from scratch\n",
    "    # numpy has a built-in method, but I implement it manually\n",
    "    total = 0\n",
    "\n",
    "    for val in y_true - y_pred:\n",
    "        total += val**2\n",
    "\n",
    "    mse = total / len(y_true)\n",
    "\n",
    "    return mse\n",
    "\n",
    "# Evaluate your custom model\n",
    "# TODO: YOUR CODE HERE: Calculate MSE for your predictions\n",
    "custom_mse = compute_mse(y_test.to_numpy(), predictions)\n",
    "print(f'Mean Squared Error (Custom Implementation): {custom_mse}')\n",
    "\n",
    "# Compare with sklearn's implementation\n",
    "# I had to rename the import because it conflicted with the custom LinearRegression class\n",
    "sklearn_model = SkLinearRegression()\n",
    "# TODO: YOUR CODE HERE: Fit sklearn model and get predictions\n",
    "sklearn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "sklearn_predictions = sklearn_model.predict(X_test_scaled)\n",
    "sklearn_mse = compute_mse(y_test.to_numpy(), sklearn_predictions)\n",
    "\n",
    "print(f'Mean Squared Error (Sklearn Implementation): {sklearn_mse}')\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COEFFICIENT COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(\"Custom Implementation:\")\n",
    "print(f\"Bias: {lr_model.bias}\")\n",
    "print(f\"Weights: {lr_model.weights}\")\n",
    "print(\"\\nSklearn Implementation:\")\n",
    "print(f\"Bias: {sklearn_model.intercept_}\")\n",
    "print(f\"Weights: {sklearn_model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e19bb",
   "metadata": {},
   "source": [
    "### 1.5 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the most important features\n",
    "# TODO: YOUR CODE HERE: Calculate feature importance based on absolute values of weights\n",
    "importance = np.abs(lr_model.weights.flatten())  # Get absolute values of weights\n",
    "feature_names = X_train_scaled.columns  # Get feature names (excluding 'Price' and 'Status')\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)  # YOUR CODE HERE: Create list of (feature_name, importance) tuples and sort\n",
    "\n",
    "print(\"Feature Importance (from most to least important):\")\n",
    "print(\"=\"*50)\n",
    "for i, (feature, imp) in enumerate(feature_importance):\n",
    "    print(f\"{i+1}. {feature}: {imp:.4f}\")\n",
    "\n",
    "print(f\"\\nThe two most significant features for predicting house price are:\")\n",
    "print(f\"1. {feature_importance[0][0]}\")\n",
    "print(f\"2. {feature_importance[1][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c024e",
   "metadata": {},
   "source": [
    "**TODO for students**: Think about wether this feature importance makes sense in the context of real estate pricing\n",
    "Does using absolute weights a good measure of feature importance? Why or why not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbe022",
   "metadata": {},
   "source": [
    "### 1.6 Repeat for Other Status Types\n",
    "\n",
    "**TODO for students:** Repeat the above analysis for \"Foreclosure\" and \"Regular\" status types.\n",
    "Create similar code blocks below for each status type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ff9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE: Implement analysis for \"Foreclosure\" status\n",
    "print(\"FORECLOSURE ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "# Copy and modify the code from above sections\n",
    "\n",
    "# I just copied everything and put it into this function\n",
    "def analyze_foreclosure():\n",
    "    X = foreclosure_data.drop(columns=['Price', 'Status'])  # Features (exclude 'Price' and 'Status')\n",
    "    y = foreclosure_data[[\"Price\"]]  # Target ('Price')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train, X_test = preprocess_location_feature(X_train, X_test, y_train)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train your custom linear regression model\n",
    "    lr_model = LinearRegression()\n",
    "\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "    custom_mse = compute_mse(y_test.to_numpy(), predictions)\n",
    "    print(f'Mean Squared Error (Custom Implementation): {custom_mse}')\n",
    "\n",
    "    # Compare with sklearn's implementation\n",
    "    sklearn_model = SkLinearRegression()\n",
    "    # TODO: YOUR CODE HERE: Fit sklearn model and get predictions\n",
    "    sklearn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    sklearn_predictions = sklearn_model.predict(X_test_scaled)\n",
    "    sklearn_mse = compute_mse(y_test.to_numpy(), sklearn_predictions)\n",
    "\n",
    "    print(f'Mean Squared Error (Sklearn Implementation): {sklearn_mse}')\n",
    "\n",
    "    # Compare coefficients\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COEFFICIENT COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Custom Implementation:\")\n",
    "    print(f\"Bias: {lr_model.bias}\")\n",
    "    print(f\"Weights: {lr_model.weights}\")\n",
    "    print(\"\\nSklearn Implementation:\")\n",
    "    print(f\"Bias: {sklearn_model.intercept_}\")\n",
    "    print(f\"Weights: {sklearn_model.coef_}\")\n",
    "\n",
    "    # Determine the most important features\n",
    "    importance = np.abs(lr_model.weights.flatten())  # Get absolute values of weights\n",
    "    feature_names = X_train_scaled.columns  # Get feature names (excluding 'Price' and 'Status')\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)  # YOUR CODE HERE: Create list of (feature_name, importance) tuples and sort\n",
    "\n",
    "    print(\"\\nFeature Importance (from most to least important):\")\n",
    "    print(\"=\"*50)\n",
    "    for i, (feature, imp) in enumerate(feature_importance):\n",
    "        print(f\"{i+1}. {feature}: {imp:.4f}\")\n",
    "\n",
    "    print(f\"\\nThe two most significant features for predicting house price are:\")\n",
    "    print(f\"1. {feature_importance[0][0]}\")\n",
    "    print(f\"2. {feature_importance[1][0]}\")\n",
    "\n",
    "analyze_foreclosure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE: Implement analysis for \"Regular\" status  \n",
    "print(\"REGULAR ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "# Copy and modify the code from above sections\n",
    "\n",
    "# same as above, but with the regular data\n",
    "def analyze_regular():\n",
    "    X = regular_data.drop(columns=['Price', 'Status'])  # Features (exclude 'Price' and 'Status')\n",
    "    y = regular_data[[\"Price\"]]  # Target ('Price')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train, X_test = preprocess_location_feature(X_train, X_test, y_train)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train your custom linear regression model\n",
    "    lr_model = LinearRegression()\n",
    "\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "    custom_mse = compute_mse(y_test.to_numpy(), predictions)\n",
    "    print(f'Mean Squared Error (Custom Implementation): {custom_mse}')\n",
    "\n",
    "    # Compare with sklearn's implementation\n",
    "    sklearn_model = SkLinearRegression()\n",
    "    # TODO: YOUR CODE HERE: Fit sklearn model and get predictions\n",
    "    sklearn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    sklearn_predictions = sklearn_model.predict(X_test_scaled)\n",
    "    sklearn_mse = compute_mse(y_test.to_numpy(), sklearn_predictions)\n",
    "\n",
    "    print(f'Mean Squared Error (Sklearn Implementation): {sklearn_mse}')\n",
    "\n",
    "    # Compare coefficients\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"COEFFICIENT COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Custom Implementation:\")\n",
    "    print(f\"Bias: {lr_model.bias}\")\n",
    "    print(f\"Weights: {lr_model.weights}\")\n",
    "    print(\"\\nSklearn Implementation:\")\n",
    "    print(f\"Bias: {sklearn_model.intercept_}\")\n",
    "    print(f\"Weights: {sklearn_model.coef_}\")\n",
    "\n",
    "    # Determine the most important features\n",
    "    importance = np.abs(lr_model.weights.flatten())  # Get absolute values of weights\n",
    "    feature_names = X_train_scaled.columns  # Get feature names (excluding 'Price' and 'Status')\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)  # YOUR CODE HERE: Create list of (feature_name, importance) tuples and sort\n",
    "\n",
    "    print(\"\\nFeature Importance (from most to least important):\")\n",
    "    print(\"=\"*50)\n",
    "    for i, (feature, imp) in enumerate(feature_importance):\n",
    "        print(f\"{i+1}. {feature}: {imp:.4f}\")\n",
    "\n",
    "    print(f\"\\nThe two most significant features for predicting house price are:\")\n",
    "    print(f\"1. {feature_importance[0][0]}\")\n",
    "    print(f\"2. {feature_importance[1][0]}\")\n",
    "\n",
    "analyze_regular()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b47a52",
   "metadata": {},
   "source": [
    "### 1.7 Summary and Comparison\n",
    "\n",
    "**TODO for students:** Write a brief summary comparing the results across all three status types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE: Create a summary comparison\n",
    "print(\"SUMMARY COMPARISON ACROSS STATUS TYPES\")\n",
    "print(\"=\"*50)\n",
    "# Compare MSE values, important features, etc.\n",
    "print(\"The MSE for Regular was higher than the MSE for Short Sale, which was higher than the MSE for Foreclosure.\")\n",
    "print(\"Size and Price/SQ.Ft were the most important features for every status type. Size was more important for Foreclosure, while Price/SQ.Ft was\")\n",
    "print(\"more important for Regular and Short Sale. These results make sense because larger houses generally cost more, and Price/SQ.Ft directly relates to the overall price.\")\n",
    "print(\"Location was always the 3rd most important feature, likely because it was replaced with the average price for all houses at that location,\")\n",
    "print(\"so it captures more information about the original price compared to features like Bedrooms or Bathrooms.\")\n",
    "print(\"MLS was always the 5th or 6th most important feature, which makes sense because it has nothing to do with the price at all,\")\n",
    "print(\"but it's interesting that MLS was more important than Bathrooms for for Regular.\")\n",
    "print(\"Bedrooms and Bathrooms probably had low weights because a house with more Size usually has more Bedrooms and Bathrooms, so Size already captures most of that information.\")\n",
    "print(\"For all statuses, the MSE for the custom model was very close to the MSE for the sklearn model (besides minor floating point differences), which indicates\")\n",
    "print(\"that the custom implementation was implemented correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73668054",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 2: Principal Component Analysis (5 pts)\n",
    "\n",
    "In this question, you will implement PCA from scratch and use it for face recognition on the Yale Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33507517",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896e489",
   "metadata": {},
   "source": [
    "### 2.1 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e88fc9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_yale_faces(data_dir=\"hw1/yalefaces\"):\n",
    "    \"\"\"\n",
    "    Load Yale face images\n",
    "    \"\"\"\n",
    "    # Steps:\n",
    "    # 1. Find all .gif files in the directory\n",
    "    # 2. Separate training images from test images (test images contain 'test' in filename)\n",
    "    # 3. Load images using plt.imread()\n",
    "    # 4. Downsample images by factor of 4\n",
    "    \n",
    "    train_imgs = []\n",
    "    test_imgs = []\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    if os.path.exists(data_dir):\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".gif\"):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    img = plt.imread(filepath)\n",
    "                    \n",
    "                    # TODO: Downsample by factor of 4 \n",
    "                    img_downsampled = img[::4, ::4] # YOUR CODE HERE\n",
    "                    \n",
    "                    if 'test' in file:\n",
    "                        test_imgs.append(img_downsampled)\n",
    "                        test_files.append(file)\n",
    "                    else:\n",
    "                        train_imgs.append(img_downsampled)\n",
    "                        train_files.append(file)\n",
    "    \n",
    "    return train_imgs, test_imgs, train_files, test_files\n",
    "\n",
    "# Load the images\n",
    "data_path = './yalefaces' # Your path to yalefaces\n",
    "train_images, test_images, train_filenames, test_filenames = load_yale_faces(data_dir=data_path)\n",
    "\n",
    "print(f\"Number of training images: {len(train_images)}\")\n",
    "print(f\"Number of test images: {len(test_images)}\")\n",
    "print(\"Training files:\", train_filenames)\n",
    "print(\"Test files:\", test_filenames)\n",
    "\n",
    "# Display sample images\n",
    "if len(train_images) > 0:\n",
    "    height, width = train_images[0].shape\n",
    "    print(f\"Image dimensions: {height} x {width}\")\n",
    "    \n",
    "    # Show a few sample images\n",
    "    fig, axes = plt.subplots(1, min(4, len(train_images)), figsize=(12, 3))\n",
    "    if len(train_images) == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(min(4, len(train_images))):\n",
    "        axes[i].imshow(train_images[i], cmap='gray')\n",
    "        axes[i].set_title(f\"Training Image {i+1}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0da6f",
   "metadata": {},
   "source": [
    "### 2.2 PCA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa195d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"Principal Component Analysis implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.mean = None\n",
    "        self.components = None\n",
    "        self.eigenvalues = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA on the data\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # TODO: YOUR CODE HERE: Implement PCA fitting\n",
    "        # Steps:\n",
    "        # 1. Center the data by subtracting the mean\n",
    "        # 2. Compute covariance matrix or use SVD\n",
    "        # 3. Find eigenvalues and eigenvectors\n",
    "        # 4. Sort by eigenvalues in descending order\n",
    "        # 5. Select top n_components eigenvectors\n",
    "\n",
    "        # using SVD because the covariance matrix was taking several minutes to compute\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "        eigenvalues = (S**2) / (X.shape[0] - 1)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "        # select the top components based on the sorted eigenvalue indices\n",
    "        self.components = Vt[idx][:self.n_components, :]\n",
    "        self.eigenvalues = eigenvalues[idx][:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to PCA space\n",
    "        \n",
    "        Args:\n",
    "            X: Data to transform\n",
    "            \n",
    "        Returns:\n",
    "            Transformed data in PCA space\n",
    "        \"\"\"\n",
    "        # TODO: YOUR CODE HERE: Implement transformation\n",
    "        # Steps:\n",
    "        # 1. Center the data using the fitted mean\n",
    "        # 2. Project onto principal components\n",
    "\n",
    "        return (X - self.mean) @ self.components.transpose()       \n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a5e9d",
   "metadata": {},
   "source": [
    "### 2.3 Eigenface Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce8205",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Convert images to feature vectors\n",
    "# TODO: YOUR CODE HERE: Convert images to vectors and create data matrix\n",
    "if len(train_images) > 0:\n",
    "    height, width = train_images[0].shape\n",
    "    \n",
    "    # Shape: (n_samples, n_features) where n_features = height * width\n",
    "    X_train = np.array([img.flatten() for img in train_images])\n",
    "    \n",
    "    print(f\"Data matrix shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bf2c5",
   "metadata": {},
   "source": [
    "### 2.4 Subject-wise PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fcd96",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_subject(subject_id, train_images, train_filenames, n_components=6):\n",
    "    \"\"\"\n",
    "    Analyze a specific subject using PCA\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE: \n",
    "    # 1. Filter images for the specific subject\n",
    "    # 2. Convert to feature vectors\n",
    "    # 3. Apply PCA\n",
    "    # 4. Extract and visualize eigenfaces\n",
    "    \n",
    "    print(f\"Analyzing Subject {subject_id}\")\n",
    "    \n",
    "    # Filter images for this subject\n",
    "    subject_images = [img for img, fname in zip(train_images, train_filenames) if f\"subject{subject_id}\" in fname]\n",
    "    # YOUR CODE HERE: Filter train_images for the specific subject\n",
    "    \n",
    "    if len(subject_images) == 0:\n",
    "        print(f\"No images found for subject {subject_id}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Found {len(subject_images)} images for subject {subject_id}\")\n",
    "    \n",
    "    # Convert to feature vectors\n",
    "    X_subject = np.array([img.flatten() for img in subject_images])  # YOUR CODE HERE\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    # YOUR CODE HERE: Fit PCA\n",
    "    pca.fit(X_subject)\n",
    "\n",
    "    # Visualize eigenfaces\n",
    "    # YOUR CODE HERE: Reshape components back to image dimensions and plot\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_components, figsize=(12, 8))\n",
    "    for i in range(n_components):\n",
    "        eigenface = pca.components[i].reshape(height, width)\n",
    "        axes[i].imshow(eigenface, cmap='gray')\n",
    "        axes[i].set_title(f\"Eigenface {i+1}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, subject_images\n",
    "\n",
    "# Analyze Subject 01\n",
    "print(\"=\"*50)\n",
    "pca_subject01, images_subject01 = analyze_subject(\"01\", train_images, train_filenames)\n",
    "\n",
    "# Analyze Subject 14  \n",
    "print(\"=\"*50)\n",
    "pca_subject14, images_subject14 = analyze_subject(\"14\", train_images, train_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a4e60",
   "metadata": {},
   "source": [
    "### 2.5 Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(test_image, test_filename, pca_models, subject_ids):\n",
    "    \"\"\"\n",
    "    Perform face recognition using PCA\n",
    "    \n",
    "    Args:\n",
    "        test_image: Test image to recognize\n",
    "        test_filename: Filename of test image\n",
    "        pca_models: Dictionary of PCA models for each subject\n",
    "        subject_ids: List of subject IDs\n",
    "        \n",
    "    Returns:\n",
    "        Recognition scores for each subject\n",
    "    \"\"\"\n",
    "    print(f\"\\nRecognizing {test_filename}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Convert test image to vector\n",
    "    test_vector = test_image.flatten().reshape(1, -1) # YOUR CODE HERE\n",
    "    scores = {}\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        if pca_models[subject_id] is not None:\n",
    "            # TODO: YOUR CODE HERE: \n",
    "            # 1. Project test image using this subject's PCA\n",
    "            # 2. Calculate reconstruction error or similarity score\n",
    "            # 3. Store the score\n",
    "            pca = pca_models[subject_id]\n",
    "            projected = pca.transform(test_vector)\n",
    "            \n",
    "            # reconstruct the image to calculate reconstruction error/similarity\n",
    "            reconstructed = projected @ pca.components + pca.mean\n",
    "\n",
    "            # reconstruction_error = np.linalg.norm(test_vector - reconstructed, 2)\n",
    "            cosine_similarity = np.dot(test_vector.flatten(), reconstructed.flatten()) / (np.linalg.norm(test_vector) * np.linalg.norm(reconstructed))\n",
    "\n",
    "            # reconstruction error is not as intuitive as the cosine similarity\n",
    "            score = cosine_similarity  # YOUR CODE HERE: Calculate appropriate score\n",
    "            scores[subject_id] = score\n",
    "            print(f\"Score for Subject {subject_id}: {score:.4f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Prepare PCA models dictionary\n",
    "pca_models = {\n",
    "    \"01\": pca_subject01,\n",
    "    \"14\": pca_subject14\n",
    "}\n",
    "\n",
    "# Recognize test images\n",
    "subject_ids = [\"01\", \"14\"]\n",
    "recognition_results = {}\n",
    "\n",
    "for i, (test_img, test_file) in enumerate(zip(test_images, test_filenames)):\n",
    "    scores = recognize_face(test_img, test_file, pca_models, subject_ids)\n",
    "    recognition_results[test_file] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a002a",
   "metadata": {},
   "source": [
    "### 2.6 Analysis and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFACE RECOGNITION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# TODO: YOUR CODE HERE: Analyze and report the four scores as requested:\n",
    "# (1) Subject 1 test image using Subject 1 eigenfaces\n",
    "# (2) Subject 1 test image using Subject 14 eigenfaces  \n",
    "# (3) Subject 14 test image using Subject 1 eigenfaces\n",
    "# (4) Subject 14 test image using Subject 14 eigenfaces\n",
    "\n",
    "# there's an extra test face for subject 14\n",
    "\n",
    "for test_file, scores in recognition_results.items():\n",
    "    print(f\"\\nTest Image: {test_file}\")\n",
    "    for subject_id, score in scores.items():\n",
    "        print(f\"  Using Subject {subject_id} eigenfaces: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392df6f",
   "metadata": {},
   "source": [
    "### 2.7 Conclusion and Discussion\n",
    "\n",
    "**TODO for students:** Based on your results, answer the following questions:\n",
    "\n",
    "1. Can you recognize faces using the computed scores? How?\n",
    "2. What do the scores tell you about the similarity between test images and different subjects?\n",
    "3. How might you improve the face recognition accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637970be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE: Write your analysis and conclusions\n",
    "print(\"ANALYSIS AND CONCLUSIONS\")\n",
    "print(\"=\"*30)\n",
    "print(\"\"\"\n",
    "1. Face Recognition Analysis:\n",
    "   The faces can be recognized using the computed scores. Because the scores represent the cosine similarity between\n",
    "   the test image and its reconstruction using the PCA model of a particular subject, a higher score indicates a better \n",
    "   match for that subject. In the results, the PCA model for a subject always yielded the highest score for test images \n",
    "   matching that subject (~0.98 compared to ~0.91), so the faces could be recognized based on if their score exceeds \n",
    "   a certain threshold (e.g., 0.95).\n",
    "\n",
    "2. Score Interpretation:\n",
    "   The scores represent the cosine similarity between the test image and its reconstruction using the PCA model of each subject.\n",
    "   A higher score indicates a better match for that subject (with 1 being a perfect match), while a lower score indicates a poorer match.\n",
    "   Very low scores would indicate that the test image doesn't resemble the subject at all.\n",
    "\n",
    "3. Potential Improvements:\n",
    "   We could increase the number of principal components to capture more variance in the training data, or we could use more training images\n",
    "   for each subject to make the PCA model more robust. Additionally, different scoring metrics could be used to differentiate between subjects more effectively.\n",
    "   Cosine similarity yielded a ~0.91 similarity score for the two subjects, which is somewhat high since they don't look very similar.\n",
    "   I experimented with using reconstruction error (L2 norm) instead of cosine similarity, which yielded a difference of ~3000 for\n",
    "   the two subjects (with the right subject being scored around ~1500), but that scoring metric wasn't as intuitive.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299002b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure you have completed:\n",
    "\n",
    "**Question 1:**\n",
    "- [ ] Loaded and explored the dataset\n",
    "- [ ] Implemented MinMaxScaler from scratch\n",
    "- [ ] Handled categorical features (Location)\n",
    "- [ ] Implemented LinearRegression from scratch\n",
    "- [ ] Evaluated model performance using MSE\n",
    "- [ ] Identified the most important features\n",
    "- [ ] Repeated analysis for all three status types\n",
    "- [ ] Provided comparison summary\n",
    "\n",
    "**Question 2:**\n",
    "- [ ] Loaded and preprocessed Yale face images\n",
    "- [ ] Implemented PCA from scratch\n",
    "- [ ] Generated eigenfaces for both subjects\n",
    "- [ ] Performed face recognition on test images\n",
    "- [ ] Reported all four required scores\n",
    "- [ ] Provided analysis and conclusions\n",
    "\n",
    "**General:**\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Results are clearly displayed and interpreted\n",
    "- [ ] Code is well-commented and readable"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
